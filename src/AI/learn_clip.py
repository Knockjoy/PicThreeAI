# -*- coding: utf-8 -*-
"""learn_CLIP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RlaOetJ8CSlGT3mh91PO5wFeb6h6suJy

# Models available
reference

- https://zero2one.jp/learningblog/what-is-clip/

- https://huggingface.co/openai/clip-vit-base-patch32

See [hugging Face](https://huggingface.co/) for availble models

this sample use the [openai/clip-vit-base-patch32](https://huggingface.co/openai/clip-vit-base-patch32) of model.
"""
# %%
#@title StartUp
from PIL import Image,ImageFile
from io import BytesIO
from transformers import CLIPProcessor, CLIPModel

model_name="openai/clip-vit-base-patch32"# @param {type:"string"}
processer_name="openai/clip-vit-base-patch32"# @param {type:"string"}

# load a CLIP model and a CLIP processer
model = CLIPModel.from_pretrained(model_name)
processer=CLIPProcessor.from_pretrained(processer_name)

def called():
  return 0

def analyzeImage(ele:list,img:ImageFile):
  elements=ele
  image=img

  # input
  inputs=processer(text=elements,images=image,return_tensors="pt",padding=True)

  # モデル推論
  outputs = model(**inputs)
  logits_per_image = outputs.logits_per_image  #calculation the score
  probs = logits_per_image.softmax(dim=1)  # change to epx

  return {"elements":elements,"score":probs[0]}

# %%
